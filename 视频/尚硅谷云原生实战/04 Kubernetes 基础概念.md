# KubernetesåŸºç¡€æ¦‚å¿µ

## 1 æ˜¯ä»€ä¹ˆ 

![img](assets/1625452569657-833e64b2-1403-4fb6-9ee3-1e166504ccf0-163918637754433.png)

> æˆ‘ä»¬æ€¥éœ€ä¸€ä¸ªå¤§è§„æ¨¡å®¹å™¨ç¼–æ’ç³»ç»Ÿ

kuberneteså…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

- **æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡**
  Kubernetes å¯ä»¥ä½¿ç”¨ DNS åç§°æˆ–è‡ªå·±çš„ IP åœ°å€å…¬å¼€å®¹å™¨ï¼Œå¦‚æœè¿›å…¥å®¹å™¨çš„æµé‡å¾ˆå¤§ï¼Œ Kubernetes å¯ä»¥è´Ÿè½½å‡è¡¡å¹¶åˆ†é…ç½‘ç»œæµé‡ï¼Œä»è€Œä½¿éƒ¨ç½²ç¨³å®šã€‚
- **å­˜å‚¨ç¼–æ’**
  Kubernetes å…è®¸ä½ è‡ªåŠ¨æŒ‚è½½ä½ é€‰æ‹©çš„å­˜å‚¨ç³»ç»Ÿï¼Œä¾‹å¦‚æœ¬åœ°å­˜å‚¨ã€å…¬å…±äº‘æä¾›å•†ç­‰ã€‚

- **è‡ªåŠ¨éƒ¨ç½²å’Œå›æ»š**
  ä½ å¯ä»¥ä½¿ç”¨ Kubernetes æè¿°å·²éƒ¨ç½²å®¹å™¨çš„æ‰€éœ€çŠ¶æ€ï¼Œå®ƒå¯ä»¥ä»¥å—æ§çš„é€Ÿç‡å°†å®é™…çŠ¶æ€ æ›´æ”¹ä¸ºæœŸæœ›çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥è‡ªåŠ¨åŒ– Kubernetes æ¥ä¸ºä½ çš„éƒ¨ç½²åˆ›å»ºæ–°å®¹å™¨ï¼Œ åˆ é™¤ç°æœ‰å®¹å™¨å¹¶å°†å®ƒä»¬çš„æ‰€æœ‰èµ„æºç”¨äºæ–°å®¹å™¨ã€‚
- **è‡ªåŠ¨å®Œæˆè£…ç®±è®¡ç®—**
  Kubernetes å…è®¸ä½ æŒ‡å®šæ¯ä¸ªå®¹å™¨æ‰€éœ€ CPU å’Œå†…å­˜ï¼ˆRAMï¼‰ã€‚ å½“å®¹å™¨æŒ‡å®šäº†èµ„æºè¯·æ±‚æ—¶ï¼ŒKubernetes å¯ä»¥åšå‡ºæ›´å¥½çš„å†³ç­–æ¥ç®¡ç†å®¹å™¨çš„èµ„æºã€‚

- **è‡ªæˆ‘ä¿®å¤**
  Kubernetes é‡æ–°å¯åŠ¨å¤±è´¥çš„å®¹å™¨ã€æ›¿æ¢å®¹å™¨ã€æ€æ­»ä¸å“åº”ç”¨æˆ·å®šä¹‰çš„ è¿è¡ŒçŠ¶å†µæ£€æŸ¥çš„å®¹å™¨ï¼Œå¹¶ä¸”åœ¨å‡†å¤‡å¥½æœåŠ¡ä¹‹å‰ä¸å°†å…¶é€šå‘Šç»™å®¢æˆ·ç«¯ã€‚
- **å¯†é’¥ä¸é…ç½®ç®¡ç†**
  Kubernetes å…è®¸ä½ å­˜å‚¨å’Œç®¡ç†æ•æ„Ÿä¿¡æ¯ï¼Œä¾‹å¦‚å¯†ç ã€OAuth ä»¤ç‰Œå’Œ ssh å¯†é’¥ã€‚ ä½ å¯ä»¥åœ¨ä¸é‡å»ºå®¹å™¨é•œåƒçš„æƒ…å†µä¸‹éƒ¨ç½²å’Œæ›´æ–°å¯†é’¥å’Œåº”ç”¨ç¨‹åºé…ç½®ï¼Œä¹Ÿæ— éœ€åœ¨å †æ ˆé…ç½®ä¸­æš´éœ²å¯†é’¥ã€‚

> ***Kubernetes ä¸ºä½ æä¾›äº†ä¸€ä¸ªå¯å¼¹æ€§è¿è¡Œåˆ†å¸ƒå¼ç³»ç»Ÿçš„æ¡†æ¶ã€‚ Kubernetes ä¼šæ»¡è¶³ä½ çš„æ‰©å±•è¦æ±‚ã€æ•…éšœè½¬ç§»ã€éƒ¨ç½²æ¨¡å¼ç­‰ã€‚ ä¾‹å¦‚ï¼ŒKubernetes å¯ä»¥è½»æ¾ç®¡ç†ç³»ç»Ÿçš„ Canary éƒ¨ç½²ã€‚***

## 2 æ¶æ„

### 2.1 å·¥ä½œæ–¹å¼

Kubernetes **Cluster** **=** N **Master** Node **+** N **Worker** Nodeï¼šNä¸»èŠ‚ç‚¹+Nå·¥ä½œèŠ‚ç‚¹ï¼› N>=1

### 2.2 ç»„ä»¶æ¶æ„

![img](assets/1625452728905-e72041a2-cf1b-4b24-a327-7f0c3974a931-163918637754537.png)

#### 2.2.1 æ§åˆ¶å¹³é¢ç»„ä»¶ï¼ˆControl Plane Componentsï¼‰ 

æ§åˆ¶å¹³é¢çš„ç»„ä»¶å¯¹é›†ç¾¤åšå‡ºå…¨å±€å†³ç­–(æ¯”å¦‚è°ƒåº¦)ï¼Œä»¥åŠæ£€æµ‹å’Œå“åº”é›†ç¾¤äº‹ä»¶ï¼ˆä¾‹å¦‚ï¼Œå½“ä¸æ»¡è¶³éƒ¨ç½²çš„ `replicas` å­—æ®µæ—¶ï¼Œå¯åŠ¨æ–°çš„ [pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)ï¼‰ã€‚

æ§åˆ¶å¹³é¢ç»„ä»¶å¯ä»¥åœ¨é›†ç¾¤ä¸­çš„ä»»ä½•èŠ‚ç‚¹ä¸Šè¿è¡Œã€‚ ç„¶è€Œï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œè®¾ç½®è„šæœ¬é€šå¸¸ä¼šåœ¨åŒä¸€ä¸ªè®¡ç®—æœºä¸Šå¯åŠ¨æ‰€æœ‰æ§åˆ¶å¹³é¢ç»„ä»¶ï¼Œ å¹¶ä¸”ä¸ä¼šåœ¨æ­¤è®¡ç®—æœºä¸Šè¿è¡Œç”¨æˆ·å®¹å™¨ã€‚ è¯·å‚é˜…[ä½¿ç”¨ kubeadm æ„å»ºé«˜å¯ç”¨æ€§é›†ç¾¤](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/high-availability/) ä¸­å…³äºå¤š VM æ§åˆ¶å¹³é¢è®¾ç½®çš„ç¤ºä¾‹ã€‚

##### 2.2.1.1 kube-apiserver

API æœåŠ¡å™¨æ˜¯ Kubernetes [æ§åˆ¶é¢](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane)çš„ç»„ä»¶ï¼Œ è¯¥ç»„ä»¶å…¬å¼€äº† Kubernetes APIã€‚ API æœåŠ¡å™¨æ˜¯ Kubernetes æ§åˆ¶é¢çš„å‰ç«¯ã€‚

Kubernetes API æœåŠ¡å™¨çš„ä¸»è¦å®ç°æ˜¯ [kube-apiserver](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)ã€‚ kube-apiserver è®¾è®¡ä¸Šè€ƒè™‘äº†æ°´å¹³ä¼¸ç¼©ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒå¯é€šè¿‡éƒ¨ç½²å¤šä¸ªå®ä¾‹è¿›è¡Œä¼¸ç¼©ã€‚ ä½ å¯ä»¥è¿è¡Œ kube-apiserver çš„å¤šä¸ªå®ä¾‹ï¼Œå¹¶åœ¨è¿™äº›å®ä¾‹ä¹‹é—´å¹³è¡¡æµé‡ã€‚

##### 2.2.1.2 etcd

etcd æ˜¯å…¼å…·ä¸€è‡´æ€§å’Œé«˜å¯ç”¨æ€§çš„é”®å€¼æ•°æ®åº“ï¼Œå¯ä»¥ä½œä¸ºä¿å­˜ Kubernetes æ‰€æœ‰é›†ç¾¤æ•°æ®çš„åå°æ•°æ®åº“ã€‚

æ‚¨çš„ Kubernetes é›†ç¾¤çš„ etcd æ•°æ®åº“é€šå¸¸éœ€è¦æœ‰ä¸ªå¤‡ä»½è®¡åˆ’ã€‚

è¦äº†è§£ etcd æ›´æ·±å±‚æ¬¡çš„ä¿¡æ¯ï¼Œè¯·å‚è€ƒ [etcd æ–‡æ¡£](https://etcd.io/docs/)ã€‚

##### 2.2.1.3 kube-scheduler

æ§åˆ¶å¹³é¢ç»„ä»¶ï¼Œè´Ÿè´£ç›‘è§†æ–°åˆ›å»ºçš„ã€æœªæŒ‡å®šè¿è¡Œ[èŠ‚ç‚¹ï¼ˆnodeï¼‰](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)çš„ [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)ï¼Œé€‰æ‹©èŠ‚ç‚¹è®© Pod åœ¨ä¸Šé¢è¿è¡Œã€‚

è°ƒåº¦å†³ç­–è€ƒè™‘çš„å› ç´ åŒ…æ‹¬å•ä¸ª Pod å’Œ Pod é›†åˆçš„èµ„æºéœ€æ±‚ã€ç¡¬ä»¶/è½¯ä»¶/ç­–ç•¥çº¦æŸã€äº²å’Œæ€§å’Œåäº²å’Œæ€§è§„èŒƒã€æ•°æ®ä½ç½®ã€å·¥ä½œè´Ÿè½½é—´çš„å¹²æ‰°å’Œæœ€åæ—¶é™ã€‚

##### 2.2.1.4 kube-controller-manager

åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿è¡Œ [æ§åˆ¶å™¨](https://kubernetes.io/zh/docs/concepts/architecture/controller/) çš„ç»„ä»¶ã€‚

ä»é€»è¾‘ä¸Šè®²ï¼Œæ¯ä¸ª[æ§åˆ¶å™¨](https://kubernetes.io/zh/docs/concepts/architecture/controller/)éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„è¿›ç¨‹ï¼Œ ä½†æ˜¯ä¸ºäº†é™ä½å¤æ‚æ€§ï¼Œå®ƒä»¬éƒ½è¢«ç¼–è¯‘åˆ°åŒä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ï¼Œå¹¶åœ¨ä¸€ä¸ªè¿›ç¨‹ä¸­è¿è¡Œã€‚

è¿™äº›æ§åˆ¶å™¨åŒ…æ‹¬:

- èŠ‚ç‚¹æ§åˆ¶å™¨ï¼ˆNode Controllerï¼‰: è´Ÿè´£åœ¨èŠ‚ç‚¹å‡ºç°æ•…éšœæ—¶è¿›è¡Œé€šçŸ¥å’Œå“åº”
- ä»»åŠ¡æ§åˆ¶å™¨ï¼ˆJob controllerï¼‰: ç›‘æµ‹ä»£è¡¨ä¸€æ¬¡æ€§ä»»åŠ¡çš„ Job å¯¹è±¡ï¼Œç„¶ååˆ›å»º Pods æ¥è¿è¡Œè¿™äº›ä»»åŠ¡ç›´è‡³å®Œæˆ

- ç«¯ç‚¹æ§åˆ¶å™¨ï¼ˆEndpoints Controllerï¼‰: å¡«å……ç«¯ç‚¹(Endpoints)å¯¹è±¡(å³åŠ å…¥ Service ä¸ Pod)
- æœåŠ¡å¸æˆ·å’Œä»¤ç‰Œæ§åˆ¶å™¨ï¼ˆService Account & Token Controllersï¼‰: ä¸ºæ–°çš„å‘½åç©ºé—´åˆ›å»ºé»˜è®¤å¸æˆ·å’Œ API è®¿é—®ä»¤ç‰Œ

##### 2.2.1.5 cloud-controller-manager

äº‘æ§åˆ¶å™¨ç®¡ç†å™¨æ˜¯æŒ‡åµŒå…¥ç‰¹å®šäº‘çš„æ§åˆ¶é€»è¾‘çš„ [æ§åˆ¶å¹³é¢](https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-control-plane)ç»„ä»¶ã€‚ äº‘æ§åˆ¶å™¨ç®¡ç†å™¨å…è®¸æ‚¨é“¾æ¥é›†ç¾¤åˆ°äº‘æä¾›å•†çš„åº”ç”¨ç¼–ç¨‹æ¥å£ä¸­ï¼Œ å¹¶æŠŠå’Œè¯¥äº‘å¹³å°äº¤äº’çš„ç»„ä»¶ä¸åªå’Œæ‚¨çš„é›†ç¾¤äº¤äº’çš„ç»„ä»¶åˆ†ç¦»å¼€ã€‚

`cloud-controller-manager` ä»…è¿è¡Œç‰¹å®šäºäº‘å¹³å°çš„æ§åˆ¶å›è·¯ã€‚ å¦‚æœä½ åœ¨è‡ªå·±çš„ç¯å¢ƒä¸­è¿è¡Œ Kubernetesï¼Œæˆ–è€…åœ¨æœ¬åœ°è®¡ç®—æœºä¸­è¿è¡Œå­¦ä¹ ç¯å¢ƒï¼Œ æ‰€éƒ¨ç½²çš„ç¯å¢ƒä¸­ä¸éœ€è¦äº‘æ§åˆ¶å™¨ç®¡ç†å™¨ã€‚

ä¸ `kube-controller-manager` ç±»ä¼¼ï¼Œ`cloud-controller-manager` å°†è‹¥å¹²é€»è¾‘ä¸Šç‹¬ç«‹çš„ æ§åˆ¶å›è·¯ç»„åˆåˆ°åŒä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶ä¸­ï¼Œä¾›ä½ ä»¥åŒä¸€è¿›ç¨‹çš„æ–¹å¼è¿è¡Œã€‚ ä½ å¯ä»¥å¯¹å…¶æ‰§è¡Œæ°´å¹³æ‰©å®¹ï¼ˆè¿è¡Œä¸æ­¢ä¸€ä¸ªå‰¯æœ¬ï¼‰ä»¥æå‡æ€§èƒ½æˆ–è€…å¢å¼ºå®¹é”™èƒ½åŠ›ã€‚

ä¸‹é¢çš„æ§åˆ¶å™¨éƒ½åŒ…å«å¯¹äº‘å¹³å°é©±åŠ¨çš„ä¾èµ–ï¼š

- èŠ‚ç‚¹æ§åˆ¶å™¨ï¼ˆNode Controllerï¼‰: ç”¨äºåœ¨èŠ‚ç‚¹ç»ˆæ­¢å“åº”åæ£€æŸ¥äº‘æä¾›å•†ä»¥ç¡®å®šèŠ‚ç‚¹æ˜¯å¦å·²è¢«åˆ é™¤
- è·¯ç”±æ§åˆ¶å™¨ï¼ˆRoute Controllerï¼‰: ç”¨äºåœ¨åº•å±‚äº‘åŸºç¡€æ¶æ„ä¸­è®¾ç½®è·¯ç”±

- æœåŠ¡æ§åˆ¶å™¨ï¼ˆService Controllerï¼‰: ç”¨äºåˆ›å»ºã€æ›´æ–°å’Œåˆ é™¤äº‘æä¾›å•†è´Ÿè½½å‡è¡¡å™¨

#### 2.2.2 Node ç»„ä»¶ 

èŠ‚ç‚¹ç»„ä»¶åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œç»´æŠ¤è¿è¡Œçš„ Pod å¹¶æä¾› Kubernetes è¿è¡Œç¯å¢ƒã€‚

##### 2.2.2.1 kubelet

ä¸€ä¸ªåœ¨é›†ç¾¤ä¸­æ¯ä¸ª[èŠ‚ç‚¹ï¼ˆnodeï¼‰](https://kubernetes.io/zh/docs/concepts/architecture/nodes/)ä¸Šè¿è¡Œçš„ä»£ç†ã€‚ å®ƒä¿è¯[å®¹å™¨ï¼ˆcontainersï¼‰](https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/#why-containers)éƒ½ è¿è¡Œåœ¨ [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) ä¸­ã€‚

kubelet æ¥æ”¶ä¸€ç»„é€šè¿‡å„ç±»æœºåˆ¶æä¾›ç»™å®ƒçš„ PodSpecsï¼Œç¡®ä¿è¿™äº› PodSpecs ä¸­æè¿°çš„å®¹å™¨å¤„äºè¿è¡ŒçŠ¶æ€ä¸”å¥åº·ã€‚ kubelet ä¸ä¼šç®¡ç†ä¸æ˜¯ç”± Kubernetes åˆ›å»ºçš„å®¹å™¨ã€‚

##### 2.2.2.2 kube-proxy

[kube-proxy](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-proxy/) æ˜¯é›†ç¾¤ä¸­æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œçš„ç½‘ç»œä»£ç†ï¼Œ å®ç° Kubernetes [æœåŠ¡ï¼ˆServiceï¼‰](https://kubernetes.io/zh/docs/concepts/services-networking/service/) æ¦‚å¿µçš„ä¸€éƒ¨åˆ†ã€‚

kube-proxy ç»´æŠ¤èŠ‚ç‚¹ä¸Šçš„ç½‘ç»œè§„åˆ™ã€‚è¿™äº›ç½‘ç»œè§„åˆ™å…è®¸ä»é›†ç¾¤å†…éƒ¨æˆ–å¤–éƒ¨çš„ç½‘ç»œä¼šè¯ä¸ Pod è¿›è¡Œç½‘ç»œé€šä¿¡ã€‚

å¦‚æœæ“ä½œç³»ç»Ÿæä¾›äº†æ•°æ®åŒ…è¿‡æ»¤å±‚å¹¶å¯ç”¨çš„è¯ï¼Œkube-proxy ä¼šé€šè¿‡å®ƒæ¥å®ç°ç½‘ç»œè§„åˆ™ã€‚å¦åˆ™ï¼Œ kube-proxy ä»…è½¬å‘æµé‡æœ¬èº«ã€‚

![img](assets/1626605698082-bf4351dd-6751-44b7-aaf7-7608c847ea42-163918637754535.png)

## 3 kubeadmåˆ›å»ºé›†ç¾¤

> è¯·å‚ç…§ä»¥å‰Dockerå®‰è£…ã€‚å…ˆæå‰ä¸ºæ‰€æœ‰æœºå™¨å®‰è£…Docker

### 3.1 å®‰è£…kubeadm

- ä¸€å°å…¼å®¹çš„ Linux ä¸»æœºã€‚Kubernetes é¡¹ç›®ä¸ºåŸºäº Debian å’Œ Red Hat çš„ Linux å‘è¡Œç‰ˆä»¥åŠä¸€äº›ä¸æä¾›åŒ…ç®¡ç†å™¨çš„å‘è¡Œç‰ˆæä¾›é€šç”¨çš„æŒ‡ä»¤
- æ¯å°æœºå™¨ 2 GB æˆ–æ›´å¤šçš„ RAM ï¼ˆå¦‚æœå°‘äºè¿™ä¸ªæ•°å­—å°†ä¼šå½±å“ä½ åº”ç”¨çš„è¿è¡Œå†…å­˜)
- 2 CPU æ ¸æˆ–æ›´å¤š
- é›†ç¾¤ä¸­çš„æ‰€æœ‰æœºå™¨çš„ç½‘ç»œå½¼æ­¤å‡èƒ½ç›¸äº’è¿æ¥(å…¬ç½‘å’Œå†…ç½‘éƒ½å¯ä»¥)
  - **è®¾ç½®é˜²ç«å¢™æ”¾è¡Œè§„åˆ™**

- èŠ‚ç‚¹ä¹‹ä¸­ä¸å¯ä»¥æœ‰é‡å¤çš„ä¸»æœºåã€MAC åœ°å€æˆ– product_uuidã€‚è¯·å‚è§[è¿™é‡Œ](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address)äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
  - **è®¾ç½®ä¸åŒhostname**

- å¼€å¯æœºå™¨ä¸Šçš„æŸäº›ç«¯å£ã€‚è¯·å‚è§[è¿™é‡Œ](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports) äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
  - **å†…ç½‘äº’ä¿¡**

- ç¦ç”¨äº¤æ¢åˆ†åŒºã€‚ä¸ºäº†ä¿è¯ kubelet æ­£å¸¸å·¥ä½œï¼Œä½  **å¿…é¡»** ç¦ç”¨äº¤æ¢åˆ†åŒºã€‚
  - **æ°¸ä¹…å…³é—­**


#### 3.1.1 åŸºç¡€ç¯å¢ƒ

> æ‰€æœ‰æœºå™¨æ‰§è¡Œä»¥ä¸‹æ“ä½œ

```bash
#å„ä¸ªæœºå™¨è®¾ç½®è‡ªå·±çš„åŸŸå
hostnamectl set-hostname xxxx


## å°† SELinux è®¾ç½®ä¸º permissive æ¨¡å¼ï¼ˆç›¸å½“äºå°†å…¶ç¦ç”¨ï¼‰
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

#å…³é—­swap
swapoff -a  
sed -ri 's/.*swap.*/#&/' /etc/fstab

#å…è®¸ iptables æ£€æŸ¥æ¡¥æ¥æµé‡
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system
```

#### 3.1.2 å®‰è£…kubeletã€kubeadmã€kubectl

```bash
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF


sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes

sudo systemctl enable --now kubelet
```

> kubelet ç°åœ¨æ¯éš”å‡ ç§’å°±ä¼šé‡å¯ï¼Œå› ä¸ºå®ƒé™·å…¥äº†ä¸€ä¸ªç­‰å¾… kubeadm æŒ‡ä»¤çš„æ­»å¾ªç¯

### 3.2 ä½¿ç”¨kubeadmå¼•å¯¼é›†ç¾¤

#### 3.2.1 ä¸‹è½½å„ä¸ªæœºå™¨éœ€è¦çš„é•œåƒ

```bash
sudo tee ./images.sh <<-'EOF'
#!/bin/bash
images=(
kube-apiserver:v1.20.9
kube-proxy:v1.20.9
kube-controller-manager:v1.20.9
kube-scheduler:v1.20.9
coredns:1.7.0
etcd:3.4.13-0
pause:3.2
)
for imageName in ${images[@]} ; do
docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName
done
EOF
   
chmod +x ./images.sh && ./images.sh
```

#### 3.2.2 åˆå§‹åŒ–ä¸»èŠ‚ç‚¹

```bash
#æ‰€æœ‰æœºå™¨æ·»åŠ masteråŸŸåæ˜ å°„ï¼Œä»¥ä¸‹éœ€è¦ä¿®æ”¹ä¸ºè‡ªå·±çš„
echo "172.31.0.4  cluster-endpoint" >> /etc/hosts



#ä¸»èŠ‚ç‚¹åˆå§‹åŒ–
kubeadm init \
--apiserver-advertise-address=172.31.0.4 \
--control-plane-endpoint=cluster-endpoint \
--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \
--kubernetes-version v1.20.9 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=192.168.0.0/16

#æ‰€æœ‰ç½‘ç»œèŒƒå›´ä¸é‡å 
```

```bash
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join cluster-endpoint:6443 --token hums8f.vyx71prsg74ofce7 \
    --discovery-token-ca-cert-hash sha256:a394d059dd51d68bb007a532a037d0a477131480ae95f75840c461e85e2c6ae3 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join cluster-endpoint:6443 --token hums8f.vyx71prsg74ofce7 \
    --discovery-token-ca-cert-hash sha256:a394d059dd51d68bb007a532a037d0a477131480ae95f75840c461e85e2c6ae3
```

```bash
#æŸ¥çœ‹é›†ç¾¤æ‰€æœ‰èŠ‚ç‚¹
kubectl get nodes

#æ ¹æ®é…ç½®æ–‡ä»¶ï¼Œç»™é›†ç¾¤åˆ›å»ºèµ„æº
kubectl apply -f xxxx.yaml

#æŸ¥çœ‹é›†ç¾¤éƒ¨ç½²äº†å“ªäº›åº”ç”¨ï¼Ÿ
docker ps   ===   kubectl get pods -A
## è¿è¡Œä¸­çš„åº”ç”¨åœ¨dockeré‡Œé¢å«å®¹å™¨ï¼Œåœ¨k8sé‡Œé¢å«Pod
kubectl get pods -A
```

#### 3.2.3 æ ¹æ®æç¤ºç»§ç»­

> masteræˆåŠŸåæç¤ºå¦‚ä¸‹ï¼š
>
> ![img](assets/1625467146849-06edb92b-9e09-4118-a5eb-8296c23d0c7c-163918637754539.png)
>

##### 3.2.3.1 è®¾ç½®.kube/config

å¤åˆ¶ä¸Šé¢å‘½ä»¤

##### 3.2.3.2 å®‰è£…ç½‘ç»œç»„ä»¶

[calicoå®˜ç½‘](https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-more-than-50-nodes)

```bash
curl https://docs.projectcalico.org/manifests/calico.yaml -O

kubectl apply -f calico.yaml
```

> ğŸ™‹â€â™‚ï¸ è§†é¢‘è¯´ä¸‹é¢ IP éœ€è¦å’Œ --pod-network-cidr ä¸€è‡´
>
> ```text
> # The default IPv4 pool to create on startup if none exists. Pod IPs will be
>       # chosen from this range. Changing this value after installation will have
>          # no effect. This should fall within `--cluster-cidr`.
>          # - name: CALICO_IPV4POOL_CIDR
>          #   value: "192.168.0.0/16"
> ```

#### 3.2.4 åŠ å…¥nodeèŠ‚ç‚¹

```bash
kubeadm join cluster-endpoint:6443 --token x5g4uy.wpjjdbgra92s25pp \
	--discovery-token-ca-cert-hash sha256:6255797916eaee52bf9dda9429db616fcd828436708345a308f4b917d3457a22
```

> æ–°ä»¤ç‰Œ
>
> kubeadm token create --print-join-command

> ***é«˜å¯ç”¨éƒ¨ç½²æ–¹å¼ï¼Œä¹Ÿæ˜¯åœ¨è¿™ä¸€æ­¥çš„æ—¶å€™ï¼Œä½¿ç”¨æ·»åŠ ä¸»èŠ‚ç‚¹çš„å‘½ä»¤å³å¯***

> ğŸ™‹â€â™‚ï¸ æŠ¥é”™ï¼š[ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1
>
> ```shell
> net.ipv4.ip_forward = 1
> ```

#### 3.2.5 éªŒè¯é›†ç¾¤

- éªŒè¯é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
  - kubectl get nodes


#### 3.2.6 éƒ¨ç½²dashboard

##### 3.2.6.1 éƒ¨ç½²

> kuberneteså®˜æ–¹æä¾›çš„å¯è§†åŒ–ç•Œé¢
>
> https://github.com/kubernetes/dashboard

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml
```

```yaml
## Copyright 2017 The Kubernetes Authors.
#
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
#
##     http://www.apache.org/licenses/LICENSE-2.0
#
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

apiVersion: v1
kind: Namespace
metadata:
  name: kubernetes-dashboard

---

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 443
      targetPort: 8443
  selector:
    k8s-app: kubernetes-dashboard

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kubernetes-dashboard
type: Opaque

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-csrf
  namespace: kubernetes-dashboard
type: Opaque
data:
  csrf: ""

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-key-holder
  namespace: kubernetes-dashboard
type: Opaque

---

kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-settings
  namespace: kubernetes-dashboard

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
rules:
  ## Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
    verbs: ["get", "update", "delete"]
    ## Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
    verbs: ["get", "update"]
    ## Allow Dashboard to get metrics.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
    verbs: ["get"]

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
rules:
  ## Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.3.1
          imagePullPolicy: Always
          ports:
            - containerPort: 8443
              protocol: TCP
          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            ## Uncomment the following line to manually specify Kubernetes API server Host
            ## If not specified, Dashboard will attempt to auto discover the API server and connect
            ## to it. Uncomment only if the default does not work.
            ## - --apiserver-host=http://my-address:port
          volumeMounts:
            - name: kubernetes-dashboard-certs
              mountPath: /certs
              ## Create on-disk volume to store exec logs
            - mountPath: /tmp
              name: tmp-volume
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 8443
            initialDelaySeconds: 30
            timeoutSeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      volumes:
        - name: kubernetes-dashboard-certs
          secret:
            secretName: kubernetes-dashboard-certs
        - name: tmp-volume
          emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      ## Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 8000
      targetPort: 8000
  selector:
    k8s-app: dashboard-metrics-scraper

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: dashboard-metrics-scraper
  template:
    metadata:
      labels:
        k8s-app: dashboard-metrics-scraper
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      containers:
        - name: dashboard-metrics-scraper
          image: kubernetesui/metrics-scraper:v1.0.6
          ports:
            - containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              scheme: HTTP
              path: /
              port: 8000
            initialDelaySeconds: 30
            timeoutSeconds: 30
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      ## Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      volumes:
        - name: tmp-volume
          emptyDir: {}
```

##### 3.2.6.2 è®¾ç½®è®¿é—®ç«¯å£

```bash
kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard
```

> type: ClusterIP æ”¹ä¸º type: NodePort

```bash
kubectl get svc -A |grep kubernetes-dashboard
### æ‰¾åˆ°ç«¯å£ï¼Œåœ¨å®‰å…¨ç»„æ”¾è¡Œ
```

è®¿é—®ï¼š https://é›†ç¾¤ä»»æ„IP:ç«¯å£      https://139.198.165.238:32759

##### 3.2.6.3 åˆ›å»ºè®¿é—®è´¦å·

```yaml
#åˆ›å»ºè®¿é—®è´¦å·ï¼Œå‡†å¤‡ä¸€ä¸ªyamlæ–‡ä»¶ï¼› vi dash.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

```shell
kubectl apply -f dash.yaml
```

##### 3.2.6.4 ä»¤ç‰Œè®¿é—®

```bash
#è·å–è®¿é—®ä»¤ç‰Œ
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
```

```json
eyJhbGciOiJSUzI1NiIsImtpZCI6InpXSkU0TjhCUmVKQzBJaC03Nk9ES2NMZ1daRTRmQ1FMZU9rRUJ3VXRnM3MifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXgyczhmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzOTZmYjdlNS0wMjA2LTQxMjctOGQzYS0xMzRlODVmYjU0MDAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.Hf5mhl35_R0iBfBW7fF198h_klEnN6pRKfk_roAzOtAN-Aq21E4804PUhe9Rr9e_uFzLfoFDXacjJrHCuhiML8lpHIfJLK_vSD2pZNaYc2NWZq2Mso-BMGpObxGA23hW0nLQ5gCxlnxIAcyE76aYTAB6U8PxpvtVdgUknBVrwXG8UC_D8kHm9PTwa9jgbZfSYAfhOHWmZxNYo7CF2sHH-AT_WmIE8xLmB7J11vDzaunv92xoUoI0ju7OBA2WRr61bOmSd8WJgLCDcyBblxz4Wa-3zghfKlp0Rgb8l56AAI7ML_snF59X6JqaCuAcCJjIu0FUTS5DuyIObEeXY-z-Rw
```

##### 3.2.6.5 ç•Œé¢

![img](assets/1625476485187-1893393c-5e0b-4d0a-ab57-e403b3a714dd-163918637754541.png)